import torch
import os
import sys

# ================= é…ç½® =================
# æŒ‡å‘ä½ å·²ç»å­˜åœ¨çš„æœ¬åœ°æƒé‡ç›®å½•
LOCAL_T5_PATH = "output/pretrained_models/t5-v1_1-xxl" 
OUTPUT_PATH = "output/null_embed.pth"
MAX_LENGTH = 120
# =======================================

def extract_offline():
    print(f"\nğŸš€ [CPUæ¨¡å¼] æ­£åœ¨åŠ è½½æœ¬åœ° T5 æƒé‡...")
    print(f"   è·¯å¾„: {LOCAL_T5_PATH}")

    # 1. è·¯å¾„æ£€æŸ¥
    if not os.path.exists(LOCAL_T5_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è·¯å¾„ {LOCAL_T5_PATH}")
        print("   è¯·ç¡®è®¤ä½ å·²ç»ä¸Šä¼ äº†å®Œæ•´çš„ T5 æ–‡ä»¶å¤¹ã€‚")
        return

    try:
        from transformers import T5EncoderModel, T5Tokenizer
        
        # å¼ºåˆ¶ä½¿ç”¨ CPUï¼Œé¿å…å ç”¨ 3070 çš„ 8G æ˜¾å­˜
        device = "cpu"
        
        # 2. åŠ è½½ Tokenizer (æœ¬åœ°)
        print("   -> Loading Tokenizer...")
        tokenizer = T5Tokenizer.from_pretrained(LOCAL_T5_PATH, local_files_only=True)
        
        # 3. åŠ è½½ Model (æœ¬åœ°)
        print("   -> Loading Model (è¿™éœ€è¦ä¸€ç‚¹æ—¶é—´è¯»å–ç¡¬ç›˜)...")
        text_encoder = T5EncoderModel.from_pretrained(
            LOCAL_T5_PATH, 
            local_files_only=True, 
            torch_dtype=torch.float32 # CPU ç”¨ float32 å…¼å®¹æ€§æœ€å¥½
        ).to(device)
        
        text_encoder.eval()
        print("   âœ… T5 åŠ è½½æˆåŠŸï¼")

    except Exception as e:
        print(f"\nâŒ åŠ è½½å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        print("   è¯·æ£€æŸ¥æ–‡ä»¶å¤¹å†…æ˜¯å¦åŒ…å« config.json, spiece.model, pytorch_model.bin ç­‰æ‰€æœ‰æ–‡ä»¶ã€‚")
        return

    # 4. æå–ç‰¹å¾
    prompts = [""] 
    print("ğŸ”„ æ­£åœ¨æå–ç©ºæ–‡æœ¬ç‰¹å¾...")
    
    with torch.no_grad():
        text_inputs = tokenizer(
            prompts,
            padding="max_length",
            max_length=MAX_LENGTH,
            truncation=True,
            return_tensors="pt",
        )
        
        prompt_embeds = text_encoder(
            text_inputs.input_ids.to(device),
            attention_mask=text_inputs.attention_mask.to(device),
        )[0]

    # 5. ä¿å­˜
    payload = {
        "prompt_embeds": prompt_embeds, 
        "attention_mask": text_inputs.attention_mask
    }
    
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    torch.save(payload, OUTPUT_PATH)
    print(f"\nğŸ‰ æˆåŠŸï¼null_embed.pth å·²ç”Ÿæˆè‡³: {OUTPUT_PATH}")
    print(f"   Shape: {prompt_embeds.shape}")

if __name__ == "__main__":
    extract_offline()