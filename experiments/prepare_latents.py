import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import os
import sys
import glob
import argparse
from tqdm import tqdm 
from diffusers import AutoencoderKL

# ================= ğŸ›‘ 1. ç¡¬ä»¶ç¡¬çº¦æŸ (é’ˆå¯¹ 3070) =================
# ä¿®å¤ RuntimeError: GET was unable to find an engine to execute this computation
torch.backends.cudnn.enabled = False
# ==============================================================

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
# è·¯å¾„é…ç½®
HR_DIR = "../dataset/DIV2K_train_HR"      
OUTPUT_DIR = "../dataset/DIV2K_train_latents" 
VAE_PATH = "../output/pretrained_models/sd-vae-ft-ema" 

# åˆ‡ç‰‡ç­–ç•¥
CROP_SIZE = 512
STRIDE = 256  # 50% é‡å 
# ===============================================

def setup_device():
    if torch.cuda.is_available():
        return "cuda"
    return "cpu"

def load_vae(path, device):
    print(f"ğŸ”„ [Init] Loading VAE from {path}...")
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ VAE model not found at {path}")
    
    # [Fix] ä½¿ç”¨ float32 åŠ è½½ VAE
    # 3070 8G æ˜¾å­˜è¶³å¤Ÿè·‘ FP32 çš„ VAE (ä»…å ç”¨çº¦ 1.5GB)
    # è¿™èƒ½é¿å… FP16 ä¸‹çš„ cuDNN å·ç§¯ç®—æ³•æŸ¥æ‰¾é”™è¯¯
    try:
        vae = AutoencoderKL.from_pretrained(path, local_files_only=True).to(device).float().eval()
    except Exception as e:
        print(f"âš ï¸ Load failed: {e}")
        sys.exit(1)
    
    # å¼€å¯åˆ‡ç‰‡æ¨ç†ï¼Œè¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜
    vae.enable_slicing()
    print("âœ… VAE Loaded (FP32 mode) & Slicing Enabled")
    return vae

def process_single_image(img_path, vae, transform, device, save_root):
    filename = os.path.basename(img_path).split('.')[0]
    
    try:
        # 1. è¯»å–å›¾ç‰‡
        img = Image.open(img_path).convert("RGB")
        img_tensor = transform(img).unsqueeze(0) # [1, 3, H, W]
    except Exception as e:
        print(f"âŒ Error reading {img_path}: {e}")
        return 0

    _, _, h, w = img_tensor.shape
    count = 0
    
    # 2. æ»‘åŠ¨çª—å£åˆ‡ç‰‡ (Sliding Window)
    y_points = list(range(0, h - CROP_SIZE + 1, STRIDE))
    if (h - CROP_SIZE) % STRIDE != 0: 
        y_points.append(h - CROP_SIZE)
        
    x_points = list(range(0, w - CROP_SIZE + 1, STRIDE))
    if (w - CROP_SIZE) % STRIDE != 0: 
        x_points.append(w - CROP_SIZE)

    if h < CROP_SIZE or w < CROP_SIZE:
        return 0

    for y in y_points:
        for x in x_points:
            # 3. è£å‰ª (CPU)
            hr_crop = img_tensor[:, :, y:y+CROP_SIZE, x:x+CROP_SIZE]
            
            # 4. ç”Ÿæˆ LR (CPU)
            lr_crop = F.interpolate(hr_crop, scale_factor=0.25, mode='bicubic', align_corners=False)
            lr_crop = F.interpolate(lr_crop, size=(CROP_SIZE, CROP_SIZE), mode='bicubic', align_corners=False)
            
            # 5. VAE ç¼–ç  (GPU FP32)
            # [Fix] è½¬ä¸º float() è€Œä¸æ˜¯ half()ï¼Œé¿å…æŠ¥é”™
            hr_crop_gpu = hr_crop.to(device).float() 
            
            with torch.no_grad():
                dist = vae.encode(hr_crop_gpu).latent_dist
                latents = dist.sample()
                # Scaling Factor
                latents = latents * vae.config.scaling_factor
            
            # 6. ä¿å­˜ (è½¬å› CPU FP16 ä¿å­˜ä»¥èŠ‚çœç©ºé—´)
            # è™½ç„¶è®¡ç®—ç”¨ FP32ï¼Œä½†å­˜å‚¨ç”¨ FP16 æ˜¯å®‰å…¨çš„
            save_dict = {
                "lr_img": lr_crop.squeeze(0).half(),      # [3, 512, 512] FP16
                "hr_latent": latents.squeeze(0).cpu().half() # [4, 64, 64] FP16
            }
            
            save_name = f"{filename}_y{y}_x{x}.pt"
            torch.save(save_dict, os.path.join(save_root, save_name))
            count += 1
            
    return count

def main():
    device = setup_device()
    print(f"ğŸš€ Starting Offline Latent Extraction on {device}")
    print(f"   Hard Constraint: torch.backends.cudnn.enabled = {torch.backends.cudnn.enabled}")
    
    if not os.path.exists(HR_DIR):
        print(f"âŒ HR Directory not found: {HR_DIR}")
        return
        
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    vae = load_vae(VAE_PATH, device)
    
    # é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])
    
    exts = ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG']
    image_paths = []
    for ext in exts:
        image_paths.extend(glob.glob(os.path.join(HR_DIR, ext)))
    image_paths = sorted(list(set(image_paths)))
    
    if len(image_paths) == 0:
        print(f"âŒ No images found in {HR_DIR}")
        return

    print(f"ğŸ“Š Found {len(image_paths)} images. Stride={STRIDE}, Crop={CROP_SIZE}")
    print(f"ğŸ“‚ Output Dir: {OUTPUT_DIR}")
    
    total_generated = 0
    pbar = tqdm(image_paths, desc="Processing", unit="img")
    
    for img_path in pbar:
        num = process_single_image(img_path, vae, transform, device, OUTPUT_DIR)
        total_generated += num
        pbar.set_postfix({"Patches": total_generated})
        
    print(f"\nâœ… All Done! Generated {total_generated} latents.")

if __name__ == "__main__":
    main()