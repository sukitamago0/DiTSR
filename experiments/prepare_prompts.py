import torch
import os
import sys
# [å…³é”®] å¼•å…¥ transformers åº“
try:
    from transformers import T5EncoderModel, T5Tokenizer
except ImportError:
    print("âŒ ç¼ºå°‘ transformers åº“ï¼Œè¯·è¿è¡Œ: pip install transformers sentencepiece")
    sys.exit(1)

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
# [è¯·ä¿®æ”¹è¿™é‡Œ] ä½ çš„ T5 æƒé‡æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„ (ä¸è¦æŒ‡å‘å…·ä½“æ–‡ä»¶ï¼ŒæŒ‡å‘æ–‡ä»¶å¤¹ï¼)
# ä¾‹å¦‚: "../output/pretrained_models/t5-v1_1-xxl"
T5_LOCAL_PATH = "../output/pretrained_models/t5-v1_1-xxl" 

# è¾“å‡ºè·¯å¾„
OUTPUT_PATH = "../output/quality_embed.pth"

# é«˜æ¸…æç¤ºè¯ (Quality Prompt)
PROMPT = "cinematic photo, highly detailed, 4k, realistic, sharp focus, high resolution"
MAX_LENGTH = 120
# ===============================================

def main():
    print(f"ğŸš€ Starting T5 Prompt Encoding (Local Mode)...")
    print(f"ğŸ“‚ T5 Path: {T5_LOCAL_PATH}")
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(T5_LOCAL_PATH):
        print(f"âŒ Error: T5 path not found: {T5_LOCAL_PATH}")
        print("   è¯·åœ¨è„šæœ¬ä¸­ä¿®æ”¹ T5_LOCAL_PATH ä¸ºä½ å®é™…å­˜æ”¾ bin/json æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚")
        return

    # 1. å¼ºåˆ¶ CPU åŠ è½½ (3070 8G æ‰›ä¸ä½ T5-XXL)
    device = "cpu"
    print("   Using Device: CPU (To save GPU VRAM)")
    
    # 2. åŠ è½½ Tokenizer
    print("â³ Loading Tokenizer...")
    try:
        # local_files_only=True ç¡®ä¿ä¸è”ç½‘ï¼Œåªç”¨æœ¬åœ°
        tokenizer = T5Tokenizer.from_pretrained(T5_LOCAL_PATH, local_files_only=True)
    except Exception as e:
        print(f"âŒ Tokenizer load failed: {e}")
        print("   è¯·æ£€æŸ¥æ–‡ä»¶å¤¹é‡Œæ˜¯å¦æœ‰ tokenizer.json æˆ– spiece.model")
        return

    # 3. åŠ è½½ T5 Model (åˆ†ç‰‡æƒé‡ä¼šè‡ªåŠ¨å¤„ç†)
    print("â³ Loading T5-XXL Model (è¯»å–åˆ†ç‰‡æƒé‡)...")
    try:
        # low_cpu_mem_usage=True æ˜¯å…³é”®ï¼Œå®ƒèƒ½ä¼˜åŒ–åˆ†ç‰‡åŠ è½½çš„å†…å­˜å ç”¨
        # torch_dtype=torch.float32 ä¿è¯ CPU å…¼å®¹æ€§
        model = T5EncoderModel.from_pretrained(
            T5_LOCAL_PATH, 
            local_files_only=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float32 
        ).to(device).eval()
    except Exception as e:
        print(f"âŒ Model load failed: {e}")
        print("   è¯·æ£€æŸ¥æ–‡ä»¶å¤¹é‡Œæ˜¯å¦æœ‰ config.json å’Œ pytorch_model-*.bin æ–‡ä»¶")
        return
        
    print("âœ… Model Loaded Successfully!")

    # 4. Tokenize
    print(f"ğŸ”„ Processing Prompt: '{PROMPT}'")
    text_inputs = tokenizer(
        PROMPT,
        padding="max_length",
        max_length=MAX_LENGTH,
        truncation=True,
        return_tensors="pt"
    )
    
    text_input_ids = text_inputs.input_ids.to(device)
    attention_mask = text_inputs.attention_mask.to(device)
    
    # 5. Inference
    print("ğŸ”„ Encoding (Running Forward)...")
    with torch.no_grad():
        prompt_embeds = model(
            input_ids=text_input_ids,
            attention_mask=attention_mask,
        )[0] # [1, 120, 4096]
        
    print(f"âœ… Generated Embed Shape: {prompt_embeds.shape}")
    
    # 6. ä¿å­˜
    save_dict = {
        "prompt_embeds": prompt_embeds.float(), 
        "attention_mask": attention_mask
    }
    
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    torch.save(save_dict, OUTPUT_PATH)
    print(f"ğŸ’¾ Saved to {OUTPUT_PATH}")
    print("ğŸ‰ T5 ç¦»çº¿å¤„ç†å®Œæˆï¼ç°åœ¨å¯ä»¥å»è·‘è®­ç»ƒè„šæœ¬äº†ã€‚")

if __name__ == "__main__":
    main()